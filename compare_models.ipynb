{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# function to train-test-split data and treat it\n",
    "\n",
    "def split_and_treat_data(X, y, encode_cats=True, scale_nums=True, randomstate=0):\n",
    "    # splitting\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=randomstate)\n",
    "\n",
    "    # transforming numericals\n",
    "    if scale_nums:\n",
    "        transformer = MinMaxScaler().fit(X_train.select_dtypes(np.number))\n",
    "\n",
    "        cols_nums = X.select_dtypes(np.number).columns\n",
    "        \n",
    "        X_train_norm = pd.DataFrame(transformer.transform(X_train.select_dtypes(np.number)), columns=cols_nums)\n",
    "        X_test_norm = pd.DataFrame(transformer.transform(X_test.select_dtypes(np.number)), columns=cols_nums)\n",
    "    else:\n",
    "        X_train_norm = X_train.select_dtypes(np.number)\n",
    "        X_test_norm = X_test.select_dtypes(np.number)\n",
    "\n",
    "    # encoding categoricals\n",
    "    if encode_cats:\n",
    "        encoder = OneHotEncoder(drop='first', handle_unknown='ignore').fit(pd.DataFrame(X_train.select_dtypes(object)))\n",
    "\n",
    "        encoded_train = encoder.transform(pd.DataFrame(X_train.select_dtypes(object))).toarray()\n",
    "        encoded_test = encoder.transform(pd.DataFrame(X_test.select_dtypes(object))).toarray()\n",
    "\n",
    "        cols_cats = encoder.get_feature_names_out(input_features=X_train.select_dtypes(object).columns)\n",
    "\n",
    "        onehot_encoded_cats_train = pd.DataFrame(encoded_train, columns=cols_cats).astype(object)\n",
    "        onehot_encoded_cats_test = pd.DataFrame(encoded_test, columns=cols_cats).astype(object)\n",
    "    else:\n",
    "        onehot_encoded_cats_train = X_train.select_dtypes(object)\n",
    "        onehot_encoded_cats_test = X_test.select_dtypes(object)\n",
    "\n",
    "\n",
    "    # concat cats + nums back together\n",
    "    X_train_treated = pd.concat([X_train_norm, onehot_encoded_cats_train], axis=1)\n",
    "    X_test_treated = pd.concat([X_test_norm, onehot_encoded_cats_test], axis=1)\n",
    "\n",
    "    return X_train_treated.reset_index(drop=True), X_test_treated, y_train.reset_index(drop=True), y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# function to fit and evaluate a model\n",
    "\n",
    "def build_eval_model(X_train_treated, X_test_treated, y_train, y_test, model, decimals=5):\n",
    "    # predict y_test\n",
    "    model = model.fit(X_train_treated, y_train)\n",
    "    pred = model.predict(X_test_treated)\n",
    "    \n",
    "    # evaluate predictions\n",
    "    print(\n",
    "        \"accuracy:\", round(model.score(X_test_treated, y_test),decimals), \n",
    "        \"  r2:\", round(r2_score(y_test, pred),decimals), \n",
    "        \"  precision:\", round(precision_score(y_test, pred),decimals), \n",
    "        \"  recall:\", round(recall_score(y_test, pred),decimals), \n",
    "        \"  f1:\", round(f1_score(y_test, pred),decimals), \"\\n\"\n",
    "        )\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, pred)), end='')\n",
    "    # return scores-dict\n",
    "    return {\n",
    "        \"accuracy\": round(model.score(X_test_treated, y_test),decimals),\n",
    "        \"precision\": round(precision_score(y_test, pred),decimals), \n",
    "        \"recall\": round(recall_score(y_test, pred),decimals), \n",
    "        \"f1\": round(f1_score(y_test, pred),decimals)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# function for manually resampling to a size between a majority and a minority (only 2 targets possible)\n",
    "\n",
    "def resample_treated(X_train_treated, X_test_treated, y_train, y_test, resample_size, show_dists=False):\n",
    "    # concat back input and target of training data\n",
    "    train_data = pd.concat([X_train_treated, y_train], axis=1)\n",
    "\n",
    "    # split majority/minority \n",
    "    mayority = pd.Series(y_train).index[0]\n",
    "    category_0 = train_data[train_data[y_train.name] == mayority]\n",
    "    category_1 = train_data[train_data[y_train.name] != mayority]\n",
    "\n",
    "    # resample the classes\n",
    "    category_0_undersampled = resample(category_0, replace=False, n_samples = resample_size)\n",
    "    category_1_oversampled = resample(category_1, replace=True, n_samples = resample_size)\n",
    "\n",
    "    # concat majority/minority back together\n",
    "    train_data = pd.concat([category_0_undersampled, category_1_oversampled], axis=0)\n",
    "\n",
    "    # split input and target\n",
    "    X_train_resampled = train_data.drop([y_train.name], axis=1)\n",
    "    y_train_resampled = train_data[y_train.name]\n",
    "\n",
    "    # show information if flag is set to True\n",
    "    if show_dists:\n",
    "        print(f'Resampled from: {y_train.value_counts()[0]}/{y_train.value_counts()[1]} to {resample_size}/{resample_size}')\n",
    "\n",
    "    return X_train_resampled, X_test_treated, y_train_resampled, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# function for automatic resampling using SMOTE and RandomUnderSampler (by default ending up with size of 0.5 of mayority)\n",
    "\n",
    "def smote_rnd_treated(X_train_treated, X_test_treated, y_train, y_test, size=100, show_dists=False):\n",
    "    # strategy is fraction of y_train-mayority\n",
    "    mayority = pd.Series(y_train).value_counts()[0]\n",
    "    strat = size/mayority\n",
    "    \n",
    "    X_train_treated,y_train = SMOTE(sampling_strategy=strat).fit_resample(X_train_treated, y_train)\n",
    "    X_train_RND,y_train_RND = RandomUnderSampler(sampling_strategy=1.0).fit_resample(X_train_treated,y_train)\n",
    "\n",
    "    # show information if flag is set to True\n",
    "    if show_dists:\n",
    "        print(f'Resampled from: {y_train.value_counts()[0]}/{y_train.value_counts()[1]} to {y_train_RND.value_counts()[0]}/{y_train_RND.value_counts()[1]}')\n",
    "\n",
    "    return X_train_RND, X_test_treated, y_train_RND, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline function to combine multiple inputs and models\n",
    "\n",
    "def pipeline(input_data, keys, y, models, encode_cats=True, scale_nums=True, sampling=None, size=100, may_val=1, min_val=0):\n",
    "    scores = {}\n",
    "    for model_name, model in models:\n",
    "        for key in keys:\n",
    "            print('#############   Model |', model_name, '  #############   Data |', key, '  #############\\n')\n",
    "            a,b,c,d = split_and_treat_data(input_data[key], y, encode_cats=encode_cats, scale_nums=scale_nums)\n",
    "\n",
    "            # evaluate sampling-flag\n",
    "            if sampling == 'resample':\n",
    "                a,b,c2,d = resample_treated(a,b,c,d, size)\n",
    "            elif sampling == 'smote_rnd':\n",
    "                a,b,c2,d = smote_rnd_treated(a,b,c,d, size)\n",
    "            else:\n",
    "                c2 = c\n",
    "            \n",
    "            # create scores-dict and add a sampling info\n",
    "            model_id = model_name.replace(' ','')\n",
    "            identif = f'{model_id}_{key}'\n",
    "            scores[identif] = build_eval_model(a,b,c2,d, model)\n",
    "            scores[identif]['resampling'] = sampling\n",
    "            if sampling:\n",
    "                print(f'\\t\\tTraining data resampled from: {c.value_counts()[0]}/{c.value_counts()[1]} to {c2.value_counts()[0]}/{c2.value_counts()[1]}\\n')\n",
    "            else:\n",
    "                print(f'\\t\\tTraining data sample sizes: {c.value_counts()[0]}/{c.value_counts()[1]}')\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data with different features\n",
    "\n",
    "cats = pd.read_csv('files_for_lab/categorical.csv').astype(object)\n",
    "targets = pd.read_csv('files_for_lab/target.csv')\n",
    "\n",
    "# create dict for X for different numericals, set target y\n",
    "X = {}\n",
    "y = targets['TARGET_B']\n",
    "methods = ['kbest', 'rfe', 'var', 'pca']\n",
    "\n",
    "# read in feature selected data with and without categorical\n",
    "for method in methods:\n",
    "    dict_key = f'only_{method}'\n",
    "    X[dict_key] = pd.read_csv(f'files_for_lab/{method}_nums.csv')\n",
    "    X[method] = pd.concat([cats, X[dict_key]], axis=1)\n",
    "\n",
    "# read original data without feature selection\n",
    "X['all'] = pd.concat([cats, pd.read_csv(f'files_for_lab/numerical.csv')], axis=1)\n",
    "X['only_nums'] = pd.read_csv(f'files_for_lab/numerical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############   Model | Random Forest Classifier   #############   Data | only_nums   #############\n",
      "\n",
      "accuracy: 0.94743   r2: -0.05549   precision: 0.0   recall: 0.0   f1: 0.0 \n",
      "\n",
      "       0  1\n",
      "0  22599  0\n",
      "1   1254  0\t\tTraining data sample sizes: 67970/3589\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    ('Random Forest Classifier', RandomForestClassifier())\n",
    "    ]\n",
    "\n",
    "inputs = ['only_nums']\n",
    "\n",
    "scores = pipeline(X, inputs, y, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############   Model | Random Forest Classifier   #############   Data | only_nums   #############\n",
      "\n",
      "accuracy: 0.94437   r2: -0.11693   precision: 0.08046   recall: 0.00558   f1: 0.01044 \n",
      "\n",
      "       0   1\n",
      "0  22519  80\n",
      "1   1247   7\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = pipeline(X, inputs, y, models, sampling='smote_rnd', size=67970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############   Model | Random Forest Classifier   #############   Data | only_nums   #############\n",
      "\n",
      "accuracy: 0.9455   r2: -0.09421   precision: 0.11667   recall: 0.00558   f1: 0.01065 \n",
      "\n",
      "       0   1\n",
      "0  22546  53\n",
      "1   1247   7\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = pipeline(X, inputs, y, models, sampling='resample', size=67970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############   Model | Random Forest Classifier   #############   Data | only_nums   #############\n",
      "\n",
      "accuracy: 0.56555   r2: -7.72252   precision: 0.0642   recall: 0.53509   f1: 0.11465 \n",
      "\n",
      "       0     1\n",
      "0  12819  9780\n",
      "1    583   671\t\tTraining data resampled from: 67970/3589 to 1000/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = pipeline(X, inputs, y, models, sampling='resample', size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############   Model | Logistic Regression   #############   Data | only_kbest   #############\n",
      "\n",
      "accuracy: 0.59883   r2: -7.05421   precision: 0.0717   recall: 0.55502   f1: 0.127 \n",
      "\n",
      "       0     1\n",
      "0  13588  9011\n",
      "1    558   696\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Logistic Regression   #############   Data | kbest   #############\n",
      "\n",
      "accuracy: 0.6169   r2: -6.69144   precision: 0.06974   recall: 0.50957   f1: 0.1227 \n",
      "\n",
      "       0     1\n",
      "0  14076  8523\n",
      "1    615   639\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Logistic Regression   #############   Data | only_rfe   #############\n",
      "\n",
      "accuracy: 0.57754   r2: -7.48179   precision: 0.06754   recall: 0.54944   f1: 0.1203 \n",
      "\n",
      "       0     1\n",
      "0  13087  9512\n",
      "1    565   689\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Logistic Regression   #############   Data | rfe   #############\n",
      "\n",
      "accuracy: 0.6208   r2: -6.61316   precision: 0.06817   recall: 0.49043   f1: 0.11971 \n",
      "\n",
      "       0     1\n",
      "0  14193  8406\n",
      "1    639   615\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Logistic Regression   #############   Data | only_var   #############\n",
      "\n",
      "accuracy: 0.61284   r2: -6.77308   precision: 0.07087   recall: 0.52552   f1: 0.12489 \n",
      "\n",
      "       0     1\n",
      "0  13959  8640\n",
      "1    595   659\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Logistic Regression   #############   Data | var   #############\n",
      "\n",
      "accuracy: 0.61682   r2: -6.69312   precision: 0.06973   recall: 0.50957   f1: 0.12267 \n",
      "\n",
      "       0     1\n",
      "0  14074  8525\n",
      "1    615   639\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Logistic Regression   #############   Data | only_pca   #############\n",
      "\n",
      "accuracy: 0.51121   r2: -8.81336   precision: 0.05186   recall: 0.48006   f1: 0.0936 \n",
      "\n",
      "       0      1\n",
      "0  11592  11007\n",
      "1    652    602\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Logistic Regression   #############   Data | pca   #############\n",
      "\n",
      "accuracy: 0.62193   r2: -6.59043   precision: 0.06742   recall: 0.48246   f1: 0.1183 \n",
      "\n",
      "       0     1\n",
      "0  14230  8369\n",
      "1    649   605\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Logistic Regression   #############   Data | all   #############\n",
      "\n",
      "accuracy: 0.61372   r2: -6.7554   precision: 0.06654   recall: 0.48724   f1: 0.11709 \n",
      "\n",
      "       0     1\n",
      "0  14028  8571\n",
      "1    643   611\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Logistic Regression   #############   Data | only_nums   #############\n",
      "\n",
      "accuracy: 0.5959   r2: -7.11313   precision: 0.0681   recall: 0.52711   f1: 0.12061 \n",
      "\n",
      "       0     1\n",
      "0  13553  9046\n",
      "1    593   661\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Decision Tree Classifier   #############   Data | only_kbest   #############\n",
      "\n",
      "accuracy: 0.88312   r2: -1.34665   precision: 0.05094   recall: 0.06938   f1: 0.05874 \n",
      "\n",
      "       0     1\n",
      "0  20978  1621\n",
      "1   1167    87\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Decision Tree Classifier   #############   Data | kbest   #############\n",
      "\n",
      "accuracy: 0.89959   r2: -1.01587   precision: 0.07457   recall: 0.07974   f1: 0.07707 \n",
      "\n",
      "       0     1\n",
      "0  21358  1241\n",
      "1   1154   100\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Decision Tree Classifier   #############   Data | only_rfe   #############\n",
      "\n",
      "accuracy: 0.88777   r2: -1.25323   precision: 0.05392   recall: 0.06858   f1: 0.06037 \n",
      "\n",
      "       0     1\n",
      "0  21090  1509\n",
      "1   1168    86\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Decision Tree Classifier   #############   Data | rfe   #############\n",
      "\n",
      "accuracy: 0.8946   r2: -1.11603   precision: 0.05758   recall: 0.06539   f1: 0.06124 \n",
      "\n",
      "       0     1\n",
      "0  21257  1342\n",
      "1   1172    82\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Decision Tree Classifier   #############   Data | only_var   #############\n",
      "\n",
      "accuracy: 0.87494   r2: -1.51078   precision: 0.05507   recall: 0.08533   f1: 0.06694 \n",
      "\n",
      "       0     1\n",
      "0  20763  1836\n",
      "1   1147   107\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Decision Tree Classifier   #############   Data | var   #############\n",
      "\n",
      "accuracy: 0.89012   r2: -1.20609   precision: 0.06158   recall: 0.07656   f1: 0.06825 \n",
      "\n",
      "       0     1\n",
      "0  21136  1463\n",
      "1   1158    96\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Decision Tree Classifier   #############   Data | only_pca   #############\n",
      "\n",
      "accuracy: 0.77818   r2: -3.45342   precision: 0.05802   recall: 0.21132   f1: 0.09105 \n",
      "\n",
      "       0     1\n",
      "0  18297  4302\n",
      "1    989   265\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Decision Tree Classifier   #############   Data | pca   #############\n",
      "\n",
      "accuracy: 0.89347   r2: -1.13875   precision: 0.05344   recall: 0.0614   f1: 0.05714 \n",
      "\n",
      "       0     1\n",
      "0  21235  1364\n",
      "1   1177    77\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Decision Tree Classifier   #############   Data | all   #############\n",
      "\n",
      "accuracy: 0.88588   r2: -1.2911   precision: 0.05889   recall: 0.07815   f1: 0.06717 \n",
      "\n",
      "       0     1\n",
      "0  21033  1566\n",
      "1   1156    98\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Decision Tree Classifier   #############   Data | only_nums   #############\n",
      "\n",
      "accuracy: 0.88119   r2: -1.38537   precision: 0.05568   recall: 0.07895   f1: 0.0653 \n",
      "\n",
      "       0     1\n",
      "0  20920  1679\n",
      "1   1155    99\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Random Forest Classifier   #############   Data | only_kbest   #############\n",
      "\n",
      "accuracy: 0.94609   r2: -0.08242   precision: 0.07895   recall: 0.00239   f1: 0.00464 \n",
      "\n",
      "       0   1\n",
      "0  22564  35\n",
      "1   1251   3\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Random Forest Classifier   #############   Data | kbest   #############\n",
      "\n",
      "accuracy: 0.94743   r2: -0.05549   precision: 0.0   recall: 0.0   f1: 0.0 \n",
      "\n",
      "       0  1\n",
      "0  22599  0\n",
      "1   1254  0\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Random Forest Classifier   #############   Data | only_rfe   #############\n",
      "\n",
      "accuracy: 0.94575   r2: -0.08916   precision: 0.1   recall: 0.00399   f1: 0.00767 \n",
      "\n",
      "       0   1\n",
      "0  22554  45\n",
      "1   1249   5\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Random Forest Classifier   #############   Data | rfe   #############\n",
      "\n",
      "accuracy: 0.94739   r2: -0.05633   precision: 0.0   recall: 0.0   f1: 0.0 \n",
      "\n",
      "       0  1\n",
      "0  22598  1\n",
      "1   1254  0\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Random Forest Classifier   #############   Data | only_var   #############\n",
      "\n",
      "accuracy: 0.93888   r2: -0.2272   precision: 0.0678   recall: 0.01276   f1: 0.02148 \n",
      "\n",
      "       0    1\n",
      "0  22379  220\n",
      "1   1238   16\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Random Forest Classifier   #############   Data | var   #############\n",
      "\n",
      "accuracy: 0.94739   r2: -0.05633   precision: 0.0   recall: 0.0   f1: 0.0 \n",
      "\n",
      "       0  1\n",
      "0  22598  1\n",
      "1   1254  0\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Random Forest Classifier   #############   Data | only_pca   #############\n",
      "\n",
      "accuracy: 0.92961   r2: -0.41321   precision: 0.02882   recall: 0.01037   f1: 0.01525 \n",
      "\n",
      "       0    1\n",
      "0  22161  438\n",
      "1   1241   13\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Random Forest Classifier   #############   Data | pca   #############\n",
      "\n",
      "accuracy: 0.94743   r2: -0.05549   precision: 0.0   recall: 0.0   f1: 0.0 \n",
      "\n",
      "       0  1\n",
      "0  22599  0\n",
      "1   1254  0\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Random Forest Classifier   #############   Data | all   #############\n",
      "\n",
      "accuracy: 0.94659   r2: -0.07232   precision: 0.08333   recall: 0.00159   f1: 0.00313 \n",
      "\n",
      "       0   1\n",
      "0  22577  22\n",
      "1   1252   2\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "#############   Model | Random Forest Classifier   #############   Data | only_nums   #############\n",
      "\n",
      "accuracy: 0.9447   r2: -0.1102   precision: 0.12644   recall: 0.00877   f1: 0.01641 \n",
      "\n",
      "       0   1\n",
      "0  22523  76\n",
      "1   1243  11\t\tTraining data resampled from: 67970/3589 to 67970/67970\n",
      "\n",
      "CPU times: user 26min 51s, sys: 2min 44s, total: 29min 35s\n",
      "Wall time: 21min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Decision Tree Classifier', DecisionTreeClassifier()),\n",
    "    ('Random Forest Classifier', RandomForestClassifier())\n",
    "    ]\n",
    "\n",
    "scores = pipeline(X, X.keys(), y, models, sampling='smote_rnd', size=67970)\n",
    "\n",
    "# 22 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('scores.json', 'w') as f:\n",
    "    json.dump(scores, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
